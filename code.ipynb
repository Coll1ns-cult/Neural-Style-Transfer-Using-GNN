{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installing necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRVTVHMEx05-",
        "outputId": "1978a03d-b518-4322-d793-a2dcc6a64603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deeplake\n",
            "  Downloading deeplake-3.5.3.tar.gz (494 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.4.0)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.26.142-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.3)\n",
            "Collecting pathos (from deeplake)\n",
            "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m785.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake)\n",
            "  Downloading humbug-0.3.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplake) (4.65.0)\n",
            "Collecting numcodecs (from deeplake)\n",
            "  Downloading numcodecs-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt (from deeplake)\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting aioboto3>=10.4.0 (from deeplake)\n",
            "  Downloading aioboto3-11.2.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.5.6)\n",
            "Collecting aiobotocore[boto3]==2.5.0 (from aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiobotocore-2.5.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.29.77,>=1.29.76 (from aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading botocore-1.29.76-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp>=3.3.1 (from aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Collecting aioitertools>=0.5.1 (from aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake)\n",
            "  Downloading boto3-1.26.76-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->deeplake)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from humbug>=0.3.1->deeplake) (2.27.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from numcodecs->deeplake) (0.4)\n",
            "Collecting ppft>=1.7.6.6 (from pathos->deeplake)\n",
            "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.6 (from pathos->deeplake)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.2 (from pathos->deeplake)\n",
            "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.14 (from pathos->deeplake)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.29.77,>=1.29.76->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.29.77,>=1.29.76->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp>=3.3.1->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.77,>=1.29.76->aiobotocore[boto3]==2.5.0->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
            "Building wheels for collected packages: deeplake\n",
            "  Building wheel for deeplake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.5.3-py3-none-any.whl size=604378 sha256=d441f945d30e5b6db4a7e7c7b1d7bcd0a90e730631b5d215be1c57f0d5589e56\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/7d/58/5ecd13c44cd2dbcb64fe72c169a6c2580ff2b5e6dc1d82af36\n",
            "Successfully built deeplake\n",
            "Installing collected packages: pyjwt, ppft, pox, numcodecs, multidict, jmespath, frozenlist, dill, async-timeout, aioitertools, yarl, multiprocess, humbug, botocore, aiosignal, s3transfer, pathos, aiohttp, boto3, aiobotocore, aioboto3, deeplake\n",
            "Successfully installed aioboto3-11.2.0 aiobotocore-2.5.0 aiohttp-3.8.4 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.2 boto3-1.26.76 botocore-1.29.76 deeplake-3.5.3 dill-0.3.6 frozenlist-1.3.3 humbug-0.3.1 jmespath-1.0.1 multidict-6.0.4 multiprocess-0.70.14 numcodecs-0.11.0 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 pyjwt-2.7.0 s3transfer-0.6.1 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install deeplake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W63eZPzjc8Dc"
      },
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import deeplake\n",
        "import torch.utils.data as data\n",
        "from PIL import Image, ImageFile\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms, models\n",
        "from tqdm import tqdm\n",
        "device = torch.device('cuda')\n",
        "# from sampler import InfiniteSamplerWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwZ4vu9fCu7b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x6RGDWfyCt4J"
      },
      "source": [
        "# Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyeWd_9ygS7U"
      },
      "outputs": [],
      "source": [
        "LR = 1e-4\n",
        "B = 4\n",
        "LR_DECAY = 2e-7\n",
        "SCALE_FACTOR = 10\n",
        "MAX_ITER = 10000\n",
        "patch_size = 8\n",
        "patch_stride = 8\n",
        "C = 256 #channel dimension of relu3_1 for input image of 256, 256\n",
        "OUTPUT_SIZE = (64, 64)\n",
        "D = C*patch_size**2 #spatial dimensions of relu3_1 for input image of 256, 256"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DmoZ_KkhI4J"
      },
      "outputs": [],
      "source": [
        "ds_c = deeplake.load('hub://activeloop/coco-train') # Deep Lake Dataset\n",
        "\n",
        "ds_s = deeplake.load('hub://activeloop/wiki-art')\n",
        "\n",
        "\n",
        "tform = transforms.Compose([\n",
        "    transforms.ToPILImage(), \n",
        "    # Must convert to PIL image for subsequent operations to run\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.expand(3, x.shape[1], x.shape[2])), \n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "     # Must convert to pytorch tensor for subsequent operations to run\n",
        "])\n",
        "\n",
        "# content Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMx-5kougkwJ"
      },
      "outputs": [],
      "source": [
        "def train_transform():\n",
        "    transform_list = [\n",
        "        transforms.ToPILImage(), \n",
        "        transforms.Resize(size=(512, 512)),\n",
        "        transforms.RandomCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.expand(3, x.shape[1], x.shape[2]))\n",
        "    ]\n",
        "    return transforms.Compose(transform_list)\n",
        "\n",
        "def adjust_learning_rate(optimizer, iteration_count):\n",
        "    \"\"\"Imitating the original implementation\"\"\"\n",
        "    lr = LR / (1.0 + LR_DECAY * iteration_count)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Essential operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRSgwZZ5dIbx"
      },
      "outputs": [],
      "source": [
        "decoder = nn.Sequential(\n",
        "    # nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    # nn.Conv2d(512, 256, (3, 3)),\n",
        "    # nn.ReLU(),\n",
        "    # nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    # nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    # nn.Conv2d(256, 256, (3, 3)),\n",
        "    # nn.ReLU(),\n",
        "    # nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    # nn.Conv2d(256, 256, (3, 3)),\n",
        "    # nn.ReLU(),\n",
        "    # nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    # nn.Conv2d(D, 256, (3, 3)),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 3, (3, 3)),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drccWh2ekkho"
      },
      "outputs": [],
      "source": [
        "vgg = nn.Sequential(\n",
        "    nn.Conv2d(3, 3, (1, 1)),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(3, 64, (3, 3)),\n",
        "    nn.ReLU(),  # r elu1-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),  # relu1-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 128, (3, 3)),\n",
        "    nn.ReLU(),  # relu2-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),  # relu2-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU()  # relu5-4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3O8_vOot9i3"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "class GATv2Layer(nn.Module):\n",
        "    def __init__(self, in_features:int, out_features:int, \n",
        "    n_heads: int, is_concat: bool = True,\n",
        "    dropout: float  = 0.6,\n",
        "    leaky_relu_slop: float = 0.2,\n",
        "    share_weights:bool = True\n",
        "    ):\n",
        "        super(GATv2Layer, self).__init__()\n",
        "        self.is_concat = is_concat\n",
        "        self.n_heads = n_heads\n",
        "        self.share_weights = share_weights,\n",
        "    \n",
        "        if is_concat:\n",
        "            # hidden_dim = \n",
        "            assert out_features % n_heads == 0\n",
        "            self.hidden_dim = out_features//n_heads\n",
        "        else:\n",
        "            self.hidden_dim = out_features\n",
        "        \n",
        "        self.Key = nn.Linear(in_features, self.hidden_dim * n_heads, bias = False)\n",
        "        if share_weights:\n",
        "            self.Query = self.Key\n",
        "        else:\n",
        "            self.Query = nn.Linear(in_features, self.hidden_dim*n_heads, bias = False)\n",
        "        \n",
        "        self.attn = nn.Linear(self.hidden_dim, 1, bias = False)\n",
        "        self.activation = nn.LeakyReLU(leaky_relu_slop)\n",
        "        self.softmax = nn.Softmax(dim = 2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, h, adj_mat):\n",
        "        adj_mat = adj_mat.unsqueeze(3)\n",
        "        if type(h) is tuple:\n",
        "            content, style = h\n",
        "            assert content.shape[0] == style.shape[0]\n",
        "            h = torch.cat((content, style), dim = 1)\n",
        "        h_shape0 = h.shape[0]\n",
        "        num_nodes = h.shape[1] #since h = NxLxF, where N is batch, L is node, and F is feature vector dimensions respectively\n",
        "        assert (True in torch.isnan(h)) is False\n",
        "        key = self.Key(h).view(-1, num_nodes, self.n_heads, self.hidden_dim).to(device)\n",
        "        query = self.Query(h).view(-1, num_nodes, self.n_heads, self.hidden_dim).to(device)\n",
        "        # del h\n",
        "        assert (True in torch.isnan(key)) is False\n",
        "        # gc.collect()\n",
        "        assert (True in torch.isnan(query)) is False\n",
        "        torch.cuda.empty_cache()\n",
        "        key_repeat = key.repeat(1, num_nodes, 1, 1) #change: repeat(1, 1, num_nodes, 1, 1) --> repeat(1, num_nodes, 1, 1)\n",
        "        # del key\n",
        "        # gc.collect()\n",
        "        query_repeat = query.repeat_interleave(num_nodes, dim = 1).to(device)\n",
        "        g_sum = (key_repeat + query_repeat).to(device)\n",
        "        g_sum = g_sum.view(-1, num_nodes, num_nodes, self.n_heads, self.hidden_dim).to(device)\n",
        "\n",
        "\n",
        "        score = self.attn(self.activation(g_sum)).squeeze(-1).to(device)\n",
        "        assert (True in torch.isnan(score)) is False\n",
        "        # del g_sum\n",
        "        # gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == h_shape0\n",
        "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == num_nodes\n",
        "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == num_nodes\n",
        "        assert adj_mat.shape[3] == 1 or adj_mat.shape[3] == self.n_heads\n",
        "        score = score.masked_fill(adj_mat == 0, float('-inf')).to(device)\n",
        "\n",
        "        attention = self.softmax(score).to(device)\n",
        "        assert (True in torch.isnan(score)) is False\n",
        "\n",
        "        attention = self.dropout(attention).to(device)\n",
        "        l1, l2, l3, l4 = attention.shape\n",
        "        assert (True in torch.isnan(attention)) is False\n",
        "        attention2 = torch.eye(num_nodes).unsqueeze(0).unsqueeze(3).expand(l1, -1, -1, l4)\n",
        "        assert attention.shape == attention2.shape\n",
        "        # del score\n",
        "        # print(attention, 'ATTENTION HERE!')\n",
        "        attn_res = torch.einsum('nijh,njhf->nihf', attention, query).to(device) #check this part again\n",
        "        assert (True in torch.isnan(attn_res)) is False\n",
        "        checking = attention.clone().detach()\n",
        "        # print(checking.reshape(-1).mean(), 'CHECKING MEAN VALUE HERE')\n",
        "\n",
        "        if self.is_concat:\n",
        "            return attn_res.reshape(h_shape0, num_nodes, self.n_heads * self.hidden_dim).to(device)\n",
        "        else:\n",
        "            return attn_res.mean(dim=2).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVklP2as0uVd"
      },
      "outputs": [],
      "source": [
        "def patch2feat(image, unfold):\n",
        "    #dimension of image: NxCxHxW\n",
        "    #patch dimension: K1xK2 = K\n",
        "    #num_of_patches for image = P\n",
        "    patches = unfold(image).to(device) #image size: NxC*KxP\n",
        "    return patches.transpose(1, 2) #dimensions: NxLxC*K\n",
        "    # change: should do NxPxC*K so I will delete the first transpose\n",
        "\n",
        "def feat2patch(feature,\n",
        "              #  weight,\n",
        "               fold):\n",
        "    # if patch_size is tuple:\n",
        "    #   k1, k2 = patch_size[0], patch_size[1]\n",
        "    # else:\n",
        "    #   k1, k2 = patch_size, patch_size\n",
        "    # h, w = output_size[0], output_size[1]\n",
        "    # N, L, D = feature.size(0), feature.size(1), feature.size(2)\n",
        "    # C = D/(k1*k2)\n",
        "\n",
        "    \n",
        "    #feature dimension: PxNxC*K\n",
        "    # feature = feature.transpose(0,1)\n",
        "    #after transpose NxPxC*K\n",
        "    # weight = torch.randn((int(D), int(C), int(k1), int(k2)), requires_grad = True).to(device)\n",
        "    # bias = torch.randn((1, int(C), int(h), int(w)), requires_grad = True).to(device)\n",
        "    # feature_convolved = feature.matmul(weight.view(weight.size(0), -1).t()).transpose(1, 2).to(device)\n",
        "    # print(feature_convolved.shape)\n",
        "    # fold = torch.nn.Fold(output_size, patch_size)\n",
        "    feature_summed = fold(feature.transpose(1, 2)).to(device)\n",
        "    #  + bias.expand(N, -1 ,-1, -1).to(device)\n",
        "    return feature_summed \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIVfy6gPrjlw"
      },
      "outputs": [],
      "source": [
        "# x = torch.randn(2, 256, 32, 32)\n",
        "# unfold = nn.Unfold(patch_size, stride = patch_stride)\n",
        "# output = patch2feat(x, unfold)\n",
        "# print(output.shape)\n",
        "# fold = nn.Fold((32,32), 8, stride =8)\n",
        "# weight = weight = torch.randn((int(C*patch_size*patch_size), int(C), int(patch_size), int(patch_size))).to(device)\n",
        "# print(feat2patch(output, weight, fold).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fliLEkwx1SS9"
      },
      "outputs": [],
      "source": [
        "def knn(t, k):\n",
        "    k += 1\n",
        "    if type(t) is tuple:\n",
        "        content, style = t\n",
        "        assert style.shape[0] == content.shape[0]\n",
        "        content = content / torch.norm(content, dim=2, keepdim=True) \n",
        "        style = style / torch.norm(style, dim=2, keepdim=True)\n",
        "        nc, l, f = content.shape #nc, l, f is batch, node, and feature vector dimensions respectively\n",
        "        ns, p, f = style.shape\n",
        "        \n",
        "        all = torch.cat((content, style), dim = 1).to(device)\n",
        "\n",
        "        similarity = torch.matmul(all, all.transpose(1, 2)).to(device) # Ncx(l+p)x(l+p)\n",
        "\n",
        "        similarity[:, l:, l:] = float('-inf')\n",
        "        similarity[:, :l, :l] = float('-inf')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        _, indices = torch.topk(similarity, k, 1, True)\n",
        "        indices = indices.transpose(1,2).to(device)\n",
        "\n",
        "        total = l+p\n",
        "        adj_matrix = torch.eye(total).unsqueeze(0).expand(nc, -1, -1).to(device)\n",
        "        adj_matrix = adj_matrix.scatter_(2, indices, 1).to(device)\n",
        "\n",
        "        \n",
        "        return adj_matrix\n",
        "    else:\n",
        "        content = t\n",
        "        content = content / torch.norm(content, dim=2, keepdim=True) \n",
        "\n",
        "        nc, l, f = content.shape #nc, l, f is batch, node, and feature vector dimensions respectively\n",
        "        similarity = torch.matmul(content, content.transpose(1, 2)) # Ncx(l)x(l)\n",
        "        _, indices = torch.topk(similarity, k, 1, True)\n",
        "        indices = indices.transpose(1,2).to(device)\n",
        "        adj_matrix = torch.zeros(nc, l, l).to(device)\n",
        "        adj_matrix = adj_matrix.scatter_(2, indices, 1).to(device)\n",
        "        return adj_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo3qNO2fFK0N"
      },
      "outputs": [],
      "source": [
        "# content = torch.randn((1, 2, 1)).to(device)\n",
        "# style = torch.randn((1, 2, 1)).to(device)\n",
        "# print(content)\n",
        "# print(style)\n",
        "# matrix = torch.tensor([[[0, 0, 0, 1],\n",
        "#                         [0, 0, 1, 0],\n",
        "#                         [0, 1, 0, 0],\n",
        "#                         [1, 0, 0, 1]]])\n",
        "# print(matrix.shape)\n",
        "# gat = GATv2Layer(1, 1, 1).to(device)\n",
        "# result = gat((content, style), matrix)\n",
        "# print(result)\n",
        "# print(result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-9s0Bu9CaBU"
      },
      "outputs": [],
      "source": [
        "# test = torch.randn((4, 3, 256,256))\n",
        "# path2 = \"/content/gdrive/My Drive/vgg_normalised.pth\"\n",
        "# vgg.load_state_dict(torch.load(path2), strict=False)\n",
        "# vgg = nn.Sequential(*list(vgg.children())[:18])\n",
        "# vgg.eval()\n",
        "# output = vgg(test)\n",
        "# print(output.shape)\n",
        "# fold = nn.Fold(OUTPUT_SIZE, patch_size)\n",
        "# unfold = nn.Unfold(patch_size, stride = patch_stride)\n",
        "# output = unfold(output).transpose(1, 2) #patches of dimension: \n",
        "# print(output.shape)\n",
        "# matrix = knn(output, 5)\n",
        "# gat1 = GATv2Layer(16384, 16384, 4)\n",
        "# output = gat1(output, matrix)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmKFwW14dUz6"
      },
      "outputs": [],
      "source": [
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "\n",
        "def calc_mean_std_(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 3)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1)\n",
        "    return feat_mean, feat_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT2HJn4odXp8"
      },
      "outputs": [],
      "source": [
        "def adaptive_instance_normalization(content_feat, style_feat):\n",
        "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
        "    size = content_feat.size()\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\n",
        "\n",
        "    normalized_feat = (content_feat - content_mean.expand(\n",
        "        size)) / content_std.expand(size)\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
        "\n",
        "def adaptive_patch_normalization(x, y):\n",
        "    assert (x.shape == y.shape)\n",
        "    style_patch_mean, style_patch_std = calc_mean_std_(y)\n",
        "    content_patch_mean, content_patch_std = calc_mean_std_(x)\n",
        "    normalized_content = (x - content_patch_mean.expand(x.size())) / content_patch_std.expand(x.size())\n",
        "    return normalized_content * style_patch_std.expand(x.size()) + style_patch_mean.expand(x.size())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build the complete network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPb4p-jQdMEj"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, encoder, decoder,\n",
        "                 gat1, gat2,\n",
        "                 fold, unfold):\n",
        "        super(Net, self).__init__()\n",
        "        enc_layers = list(encoder.children())\n",
        "        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n",
        "        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
        "        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
        "        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
        "        self.decoder = decoder\n",
        "        self.gat1 = gat1\n",
        "        self.gat2 = gat2\n",
        "        self.fold = fold\n",
        "        self.unfold = unfold\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode = 'nearest')\n",
        "\n",
        "        # self.fold = nn.Fold()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "        # fix the encoder\n",
        "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n",
        "            for param in getattr(self, name).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n",
        "    def encode_with_intermediate(self, input):\n",
        "        results = [input]\n",
        "        for i in range(4):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "    # extract relu4_1 from input image\n",
        "    def encode(self, input):\n",
        "        for i in range(4):\n",
        "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
        "        return input\n",
        "\n",
        "    def calc_content_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        assert (target.requires_grad is False)\n",
        "        return self.mse_loss(input, target)\n",
        "\n",
        "    def calc_style_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        assert (target.requires_grad is False)\n",
        "        input_mean, input_std = calc_mean_std(input)\n",
        "        target_mean, target_std = calc_mean_std(target)\n",
        "        return self.mse_loss(input_mean, target_mean) + \\\n",
        "               self.mse_loss(input_std, target_std)\n",
        "\n",
        "    def forward(self, content, style, patch_size, patch_stride, k, alpha=1.0,):\n",
        "        assert 0 <= alpha <= 1\n",
        "        style_feats = self.encode_with_intermediate(style) #relu1_1, relu2_1, relu3_1, relu4_1\n",
        "        content_feats = self.encode_with_intermediate(content) #relu1_1, relu2_1, relu3_1, relu4_1\n",
        "\n",
        "        # #assigning spatial sizes of the content for feat2patch later on\n",
        "        _, _, H, W = content_feats[-2].shape\n",
        "\n",
        "        content3_1 = content_feats[-2]\n",
        "        style3_1 = style_feats[-2]\n",
        "        #patch2feat to construct the node features\n",
        "        content_patches = patch2feat(content3_1, self.unfold) #we take [-2] of feats since\n",
        "        style_patches = patch2feat(style3_1, self.unfold)\n",
        "        #knn to construct the adjacency matrix for style to content message passing\n",
        "        style_matrix = knn((content_patches, style_patches), k)\n",
        "        updated_content_patches = self.gat1((content_patches,style_patches), style_matrix)[:, :content_patches.shape[1]]\n",
        "\n",
        "        #knn to construct the adjacency matrix for content to content message passing\n",
        "        content_matrix = knn(updated_content_patches, k)\n",
        "        final_content_patches = self.gat2(updated_content_patches, content_matrix)\n",
        "\n",
        "        #feat2patch to reconstruct the imagelike content features\n",
        "        final_content_feat = feat2patch(final_content_patches,\n",
        "                                        # self.weight, \n",
        "                                        self.fold)\n",
        "  \n",
        "        #feature refinement\n",
        "        t = adaptive_instance_normalization(final_content_feat, style_feats[-2])\n",
        "        t = alpha * t + (1 - alpha) * final_content_feat\n",
        "\n",
        "        g_t = self.decoder(t)\n",
        "        g_t_feats = self.encode_with_intermediate(g_t)\n",
        "\n",
        "        t_grad_false = t.clone().detach()\n",
        "        t_grad_false.requires_grad = False\n",
        "        # print(t_grad_false.requires_grad)\n",
        "        loss_c = self.calc_content_loss(g_t_feats[-2], t_grad_false)\n",
        "        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n",
        "        for i in range(1, 4):\n",
        "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
        "        return loss_c, loss_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ifPRyup3wuV"
      },
      "outputs": [],
      "source": [
        "fold = nn.Fold((64,64), patch_size, stride = patch_stride)\n",
        "unfold = nn.Unfold(patch_size, stride = patch_stride)\n",
        "#loading parameters of vgg\n",
        "path2 = \"/content/gdrive/My Drive/vgg_normalised.pth\"\n",
        "vgg.load_state_dict(torch.load(path2), strict=False)\n",
        "vgg = nn.Sequential(*list(vgg.children())[:31])\n",
        "\n",
        "#initing parameter for fold \n",
        "# weight = torch.randn((int(C*patch_size*patch_size), int(C), int(patch_size), int(patch_size))).to(device)\n",
        "# conv_weight = nn.Parameter(weight)\n",
        "\n",
        "#initing GAT layers\n",
        "gat1 = GATv2Layer(16384, 16384, 4).to(device)\n",
        "gat2 = GATv2Layer(16384, 16384, 4).to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmDz-Rjb5FQY"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQAl9ox4dPD-"
      },
      "outputs": [],
      "source": [
        "network = Net(vgg, decoder,\n",
        "              gat1, gat2,\n",
        "              # conv_weight,\n",
        "              fold,\n",
        "              unfold)\n",
        "network.train()\n",
        "network.to(device)\n",
        "\n",
        "content_tf = train_transform()\n",
        "style_tf = train_transform()\n",
        "\n",
        "c_loader= ds_c.pytorch(batch_size = B, transform = {'images': content_tf}, shuffle = False)\n",
        "c_iter = iter(c_loader)\n",
        "\n",
        "# writer = SummaryWriter(log_dir=str(log_dir))\n",
        "\n",
        "# style dataloader\n",
        "s_loader= ds_s.pytorch(batch_size = B, transform = {'images': style_tf, 'labels': None}, shuffle = False)\n",
        "s_iter = iter(s_loader)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam([{'params':network.decoder.parameters(), 'lr':LR},\n",
        "                              {'params':network.gat1.parameters(), 'lr':LR},\n",
        "                              {'params':network.gat2.parameters(), 'lr':LR}\n",
        "                              # {'params':network.weight, 'lr':LR}\n",
        "                              ])\n",
        "\n",
        "\n",
        "for i in tqdm(range(MAX_ITER)):\n",
        "    adjust_learning_rate(optimizer, iteration_count=i)\n",
        "    content_images = next(c_iter)['images'].to(device)\n",
        "    try:\n",
        "      style_images = next(s_iter)['images'].to(device)\n",
        "    except StopIteration:\n",
        "      s_iter = iter(s_loader)\n",
        "      style_images = next(s_iter)['images'].to(device)\n",
        "    loss_c, loss_s = network(content_images, style_images, patch_size, patch_stride,3)\n",
        "    #   def forward(self, content, style, patch_size, patch_stride, k, symm, alpha=1.0,):\n",
        "    loss_c = loss_c\n",
        "    loss_s =  SCALE_FACTOR * loss_s\n",
        "    loss = loss_c + loss_s \n",
        "    torch.cuda.empty_cache()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# # #Loading decoder to drive\n",
        "# # decoder_name = 'decoder.pt'\n",
        "# # path = F\"/content/gdrive/My Drive/{decoder_name}\" \n",
        "# # torch.save(decoder.state_dict(), path)\n",
        "# # #loading GAT layers to drive\n",
        "# # gat1_name = 'gat1.pt'\n",
        "# # path = F\"/content/gdrive/My Drive/{gat1_name}\" \n",
        "# # torch.save(gat1.state_dict(), path)\n",
        "\n",
        "# # gat2_name = 'gat2.pt'\n",
        "# # path = F\"/content/gdrive/My Drive/{gat2_name}\" \n",
        "# # torch.save(gat2.state_dict(), path)\n",
        "\n",
        "# # #Loading parameters of weight:\n",
        "# # weight_name = 'weight.pt'\n",
        "# # path = F\"/content/gdrive/My Drive/{weight_name}\" \n",
        "# # torch.save(conv_weight.state_dict(), path)\n",
        "\n",
        "# #     writer.add_scalar('loss_content', loss_c.item(), i + 1)\n",
        "# #     writer.add_scalar('loss_style', loss_s.item(), i + 1)\n",
        "# # writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxOv_TYQurl3"
      },
      "outputs": [],
      "source": [
        "# x = torch.randn((1, 256, 64, 64)).to(device)\n",
        "# x_patch = patch2feat(x, unfold)\n",
        "# reconstructed_x = feat2patch(x_patch, fold)\n",
        "# (x - reconstructed_x).abs().max()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50X6WCg_-68d"
      },
      "outputs": [],
      "source": [
        "# pool = nn.MaxPool2d(2, 2)\n",
        "# upsample = nn.Upsample(scale_factor=2, mode = 'nearest')\n",
        "def graph_part(content, style):\n",
        "  content3_1 = content\n",
        "  style3_1 = style\n",
        "  assert (True in torch.isnan(content3_1)) is False\n",
        "  assert (True in torch.isnan(style3_1)) is False\n",
        "  #patch2feat to construct the node features\n",
        "  content_patches = patch2feat(content3_1, unfold) #we take [-2] of feats since\n",
        "  style_patches = patch2feat(style3_1, unfold)\n",
        "  ## knn to construct the adjacency matrix for style to content message passing\n",
        "  style_matrix = knn((content_patches, style_patches), 5).to(device)\n",
        "  content_matrix = knn(content_patches, 5).to(device)\n",
        "  assert (True in torch.isnan(content_patches)) is False\n",
        "  assert (True in torch.isnan(style_patches)) is False\n",
        "  updated_content_patches = gat1((content_patches,style_patches), style_matrix)[:, :content_patches.shape[1]]\n",
        "  assert (True in torch.isnan(updated_content_patches)) is False\n",
        "  ##knn to construct the adjacency matrix for content to content message passing\n",
        "  final_content_patches = gat2(updated_content_patches, content_matrix)\n",
        "\n",
        "  #feat2patch to reconstruct the imagelike content features\n",
        "  final_content_feat = feat2patch(content_patches,\n",
        "                                  # weight,\n",
        "                                  fold)\n",
        "  return final_content_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_2qNraCkZ9E"
      },
      "outputs": [],
      "source": [
        "def style_transfer(vgg, decoder, content, style, alpha=1.0,\n",
        "                   interpolation_weights=None):\n",
        "    assert (0.0 <= alpha <= 1.0)\n",
        "    content_f = vgg(content)\n",
        "    style_f = vgg(style)\n",
        "    content_f = graph_part(content_f, style_f)\n",
        "    if interpolation_weights:\n",
        "        _, C, H, W = content_f.size()\n",
        "        feat = torch.FloatTensor(1, C, H, W).zero_().to(device)\n",
        "        base_feat = adaptive_instance_normalization(content_f, style_f)\n",
        "        for i, w in enumerate(interpolation_weights):\n",
        "            feat = feat + w * base_feat[i:i + 1]\n",
        "            feat.to(device)\n",
        "        content_f = content_f[0:1]\n",
        "    else:\n",
        "        feat = adaptive_instance_normalization(content_f, style_f)\n",
        "    feat = feat * alpha + content_f * (1 - alpha)\n",
        "    return decoder(feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xfprCFehEENN"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "decoder.eval()\n",
        "decoder.to(device)\n",
        "\n",
        "vgg.eval()\n",
        "path2 = \"/content/gdrive/My Drive/vgg_normalised.pth\"\n",
        "vgg.load_state_dict(torch.load(path2), strict=False)\n",
        "vgg = nn.Sequential(*list(vgg.children())[:18])\n",
        "vgg.to(device)\n",
        "\n",
        "# Upload the first image file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded image file name\n",
        "image_filename1 = next(iter(uploaded))\n",
        "\n",
        "# Upload the second image file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded image file name\n",
        "image_filename2 = next(iter(uploaded))\n",
        "\n",
        "# Open the first uploaded image\n",
        "image1 = Image.open(image_filename1)\n",
        "\n",
        "# Open the second uploaded image\n",
        "image2 = Image.open(image_filename2)\n",
        "\n",
        "# Preprocess the images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(size=(256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.expand(3, x.shape[1], x.shape[2]))\n",
        "])\n",
        "\n",
        "input_tensor1 = preprocess(image1)\n",
        "input_tensor2 = preprocess(image2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh6tQWUz0tRx"
      },
      "outputs": [],
      "source": [
        " # Concatenate along the batch dimension\n",
        "\n",
        "# Assuming you have already defined and loaded your model\n",
        "content = input_tensor1.unsqueeze(0).to(device)\n",
        "style = input_tensor2.unsqueeze(0).to(device)\n",
        "assert style is not None\n",
        "\n",
        "\n",
        "output_image = style_transfer(vgg, decoder, content, style, alpha = 0.8).detach().to('cpu')\n",
        "output_image = output_image.squeeze(0).permute(1, 2, 0)\n",
        "# Show the input and output images\n",
        "fig, axs = plt.subplots(1, 3)\n",
        "axs[0].imshow(image1)\n",
        "axs[0].set_title(\"image1\")\n",
        "axs[0].axis(\"off\")\n",
        "\n",
        "axs[1].imshow(image2)\n",
        "axs[1].set_title(\"image2\")\n",
        "axs[1].axis(\"off\")\n",
        "\n",
        "axs[2].imshow(output_image)\n",
        "axs[2].set_title(\"Output Image\")\n",
        "axs[2].axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UTZnZOjhgapP"
      },
      "source": [
        "# Pyramid features (not implemented because of GPU mem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR4eh_Oggl-M"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import torch\n",
        "def pyramid_feature(image): # we use a 4 by 4 grid\n",
        "  model = models.vgg19(pretrained=True)\n",
        "  enc_1 = model.features[:18]  # relu3_1\n",
        "  enc_2 = model.features[18:27]  #relu4_1\n",
        "  feat_1 = enc_1(image)\n",
        "  feat_2 = enc_2(feat_1)\n",
        "  feat_1 = extract_patches(feat_1, 14, 14)\n",
        "  feat_2 = extract_patches(feat_2, 7, 7)\n",
        "  return torch.cat([feat_1,feat_2], dim = 2)\n",
        "  \n",
        "print(pyramid_feature(torch.randn(2, 3, 224, 224)).shape) # torch.Size([2, 16, 75264])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
